{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "!pip install torch\n",
    "\n",
    "!wget -q https://github.com/huggingface/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
    "%pip install -qq git+https://github.com/huggingface/diffusers.git\n",
    "%pip install -q -U --pre triton\n",
    "%pip install -q accelerate transformers ftfy xformers bitsandbytes==0.35.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from __future__ import print_function\n",
    "import time\n",
    "from google.colab.patches import cv2_imshow\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import Swin2SRForImageSuperResolution\n",
    "from transformers import Swin2SRImageProcessor "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True)\n",
    "# Segment people only for the purpose of human silhouette extraction\n",
    "people_class = 15\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "print (\"Model has been loaded.\")\n",
    "\n",
    "blur = torch.FloatTensor([[[[1.0, 2.0, 1.0],[2.0, 4.0, 2.0],[1.0, 2.0, 1.0]]]]) / 16.0\n",
    "\n",
    "# Use GPU if supported, for better performance\n",
    "if torch.cuda.is_available():\n",
    "\tmodel.to('cuda')\n",
    "\tblur = blur.to('cuda')\n",
    "\t\n",
    "# Apply preprocessing (normalization)\n",
    "preprocess = transforms.Compose([\n",
    "\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to create segmentation mask\n",
    "def makeSegMask(img):\n",
    "    # Scale input frame\n",
    "\tframe_data = torch.FloatTensor( img ) / 255.0\n",
    "\n",
    "\tinput_tensor = preprocess(frame_data.permute(2, 0, 1))\n",
    "    \n",
    "    # Create mini-batch to be used by the model\n",
    "\tinput_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Use GPU if supported, for better performance\n",
    "\tif torch.cuda.is_available():\n",
    "\t\tinput_batch = input_batch.to('cuda')\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(input_batch)['out'][0]\n",
    "\n",
    "\tsegmentation = output.argmax(0)\n",
    "\n",
    "\tbgOut = output[0:1][:][:]\n",
    "\ta = (1.0 - F.relu(torch.tanh(bgOut * 0.30 - 1.0))).pow(0.5) * 2.0\n",
    "\n",
    "\tpeople = segmentation.eq( torch.ones_like(segmentation).long().fill_(people_class) ).float()\n",
    "\n",
    "\tpeople.unsqueeze_(0).unsqueeze_(0)\n",
    "\t\n",
    "\tfor i in range(3):\n",
    "\t\tpeople = F.conv2d(people, blur, stride=1, padding=1)\n",
    "\n",
    "\t# Activation function to combine masks - F.hardtanh(a * b)\n",
    "\tcombined_mask = F.relu(F.hardtanh(a * (people.squeeze().pow(1.5)) ))\n",
    "\tcombined_mask = combined_mask.expand(1, 3, -1, -1)\n",
    "\n",
    "\tres = (combined_mask * 255.0).cpu().squeeze().byte().permute(1, 2, 0).numpy()\n",
    "\n",
    "\treturn res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/content/sddefault.png')\n",
    "# Apply background subtraction to extract foreground (silhouette)\n",
    "mask = makeSegMask(img)\n",
    "\n",
    "# Apply thresholding to convert mask to binary map\n",
    "ret,thresh = cv2.threshold(mask,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "# Show extracted silhouette only, by multiplying mask and input frame\n",
    "final = cv2.bitwise_and(thresh, img)\n",
    "\n",
    "# Show current frame\n",
    "cv2_imshow(mask)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Swin2SRForImageSuperResolution.from_pretrained(\"caidas/swin2SR-classical-sr-x2-64\")\n",
    "url = \"https://huggingface.co/spaces/jjourney1125/swin2sr/resolve/main/samples/butterfly.jpg\"\n",
    "\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Swin2SRImageProcessor()\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = outputs.reconstruction.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "output = np.moveaxis(output, source=0, destination=-1)\n",
    "output = (output * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
    "Image.fromarray(output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
