{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q git+https://github.com/huggingface/transformers.git\n",
    "%pip install torch\n",
    "\n",
    "!wget -q https://github.com/huggingface/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
    "%pip install -qq git+https://github.com/huggingface/diffusers.git &> /dev/null\n",
    "%pip install -q -U --pre triton &> /dev/null\n",
    "%pip install -q accelerate transformers ftfy xformers bitsandbytes==0.35.0 &> /dev/null\n",
    "\n",
    "%pip install -q diffusers==0.14.0 transformers xformers git+https://github.com/huggingface/accelerate.git\n",
    "%pip install -q opencv-contrib-python\n",
    "%pip install -q controlnet_aux==0.0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from google.colab.patches import cv2_imshow\n",
    "from transformers import Swin2SRForImageSuperResolution\n",
    "from transformers import Swin2SRImageProcessor \n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from controlnet_aux import HEDdetector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dreambooth Image Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ControlNet Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CN_pipeline:\n",
    "    def __init__(self):\n",
    "        controlnet = ControlNetModel.from_pretrained(\"fusing/stable-diffusion-v1-5-controlnet-hed\", torch_dtype=torch.float16)\n",
    "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n",
    "        )\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "        self.pipe = pipe\n",
    "    \n",
    "    def process(self, image, prompt = \"\", canny_config = \n",
    "        {\"num_inference_steps\":20, \"num_images_per_prompt\":4}):\n",
    "        if(not ('num_inference_steps' in canny_config and 'num_images_per_prompt' in canny_config)):\n",
    "            print(\"canny_config missing num_inference_steps or num_images_per_prompt\")\n",
    "            print(\"Using default canny_config\")\n",
    "            canny_config = {\"num_inference_steps\":20, \"num_images_per_prompt\":4}\n",
    "        hed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\n",
    "        hed_image = hed(image)\n",
    "        images = self.pipe(prompt,\n",
    "             hed_image,\n",
    "             num_inference_steps=int(canny_config[\"num_inference_steps\"]),\n",
    "             num_images_per_prompt=int(canny_config[\"num_images_per_prompt\"]))[0]\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calling CN \n",
    "testp = CN_pipeline()\n",
    "\n",
    "extern_img2 = download_image(\"https://www.bing.com/images/blob?bcid=qOKuNG1lPVkFfA\", False).resize((512,512))\n",
    "\n",
    "# can ignore canny config, it is built in to the class.\n",
    "canny_config = {\"num_inference_steps\":20, \"num_images_per_prompt\":4}\n",
    "prompt = \"Car on cobble street with a sunny sky\"\n",
    "images = testp.process(extern_img2,prompt=prompt,canny_config=canny_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpainting of Focus Object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True)\n",
    "# Segment people only for the purpose of human silhouette extraction\n",
    "people_class = 15\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "print (\"Model has been loaded.\")\n",
    "\n",
    "blur = torch.FloatTensor([[[[1.0, 2.0, 1.0],[2.0, 4.0, 2.0],[1.0, 2.0, 1.0]]]]) / 16.0\n",
    "\n",
    "# Use GPU if supported, for better performance\n",
    "if torch.cuda.is_available():\n",
    "\tmodel.to('cuda')\n",
    "\tblur = blur.to('cuda')\n",
    "\t\n",
    "# Apply preprocessing (normalization)\n",
    "preprocess = transforms.Compose([\n",
    "\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to create segmentation mask\n",
    "def makeSegMask(img):\n",
    "    # Scale input frame\n",
    "\tframe_data = torch.FloatTensor( img ) / 255.0\n",
    "\n",
    "\tinput_tensor = preprocess(frame_data.permute(2, 0, 1))\n",
    "    \n",
    "    # Create mini-batch to be used by the model\n",
    "\tinput_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Use GPU if supported, for better performance\n",
    "\tif torch.cuda.is_available():\n",
    "\t\tinput_batch = input_batch.to('cuda')\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(input_batch)['out'][0]\n",
    "\n",
    "\tsegmentation = output.argmax(0)\n",
    "\n",
    "\tbgOut = output[0:1][:][:]\n",
    "\ta = (1.0 - F.relu(torch.tanh(bgOut * 0.30 - 1.0))).pow(0.5) * 2.0\n",
    "\n",
    "\tpeople = segmentation.eq( torch.ones_like(segmentation).long().fill_(people_class) ).float()\n",
    "\n",
    "\tpeople.unsqueeze_(0).unsqueeze_(0)\n",
    "\t\n",
    "\tfor i in range(3):\n",
    "\t\tpeople = F.conv2d(people, blur, stride=1, padding=1)\n",
    "\n",
    "\t# Activation function to combine masks - F.hardtanh(a * b)\n",
    "\tcombined_mask = F.relu(F.hardtanh(a * (people.squeeze().pow(1.5)) ))\n",
    "\tcombined_mask = combined_mask.expand(1, 3, -1, -1)\n",
    "\n",
    "\tres = (combined_mask * 255.0).cpu().squeeze().byte().permute(1, 2, 0).numpy()\n",
    "\n",
    "\treturn res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/content/sddefault.png')\n",
    "# Apply background subtraction to extract foreground (silhouette)\n",
    "mask = makeSegMask(img)\n",
    "\n",
    "# Apply thresholding to convert mask to binary map\n",
    "ret,thresh = cv2.threshold(mask,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "# Show extracted silhouette only, by multiplying mask and input frame\n",
    "final = cv2.bitwise_and(thresh, img)\n",
    "\n",
    "# Show current frame\n",
    "cv2_imshow(mask)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/sddefault.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\Git Repos\\spyder-lite\\Submission.ipynb Cell 14\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/Git%20Repos/spyder-lite/Submission.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# model = Swin2SRForImageSuperResolution.from_pretrained(\"caidas/swin2SR-classical-sr-x2-64\")\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/Git%20Repos/spyder-lite/Submission.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/Git%20Repos/spyder-lite/Submission.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(\u001b[39m'\u001b[39;49m\u001b[39m/content/sddefault.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/Git%20Repos/spyder-lite/Submission.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m image\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\PIL\\Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2950\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   2952\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 2953\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   2954\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   2956\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sddefault.png'"
     ]
    }
   ],
   "source": [
    "model = Swin2SRForImageSuperResolution.from_pretrained(\"caidas/swin2SR-classical-sr-x2-64\")\n",
    "image = Image.open('/content/sddefault.png')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Swin2SRImageProcessor()\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = outputs.reconstruction.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "output = np.moveaxis(output, source=0, destination=-1)\n",
    "output = (output * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
    "Image.fromarray(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
